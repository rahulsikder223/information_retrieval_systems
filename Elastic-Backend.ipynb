{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install elasticsearch\n",
    "! pip install transformers\n",
    "! pip install flask\n",
    "! pip install flask-cors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch import helpers\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "tqdm.pandas()\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from flask import Flask\n",
    "from flask import request\n",
    "from flask_cors import CORS, cross_origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('arxiv_embedded_sbert_full.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = 'elastic'\n",
    "password = '3W5dPvw=_sV*D_bmxa=x'\n",
    "host = {\"scheme\": \"https\", \"host\": \"host.docker.internal\", \"port\": 9200}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch(\n",
    "    hosts=[host],\n",
    "    basic_auth=(username, password),\n",
    "    verify_certs=False\n",
    ")\n",
    "\n",
    "es.ping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_mapping = {\n",
    "    \"properties\": {\n",
    "        \"id\": {\n",
    "            \"type\": \"long\"\n",
    "        },\n",
    "        \"title\": {\n",
    "            \"type\": \"text\"\n",
    "        },\n",
    "        \"abstract\": {\n",
    "            \"type\": \"text\"\n",
    "        },\n",
    "        \"authors\": {\n",
    "            \"type\": \"text\"\n",
    "        },\n",
    "        \"doi\": {\n",
    "            \"type\": \"text\"\n",
    "        },\n",
    "        \"update_date\": {\n",
    "            \"type\": \"text\"\n",
    "        },\n",
    "        \"search_text\": {\n",
    "            \"type\": \"text\"\n",
    "        },\n",
    "        \"abstract_vector\": {\n",
    "            \"type\": \"dense_vector\",\n",
    "            \"dims\": 768,\n",
    "            \"index\": True,\n",
    "            \"similarity\": \"cosine\",\n",
    "            \"index_options\": {\n",
    "                \"type\": \"hnsw\",\n",
    "                \"ef_construction\": 128,\n",
    "                \"m\": 24\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es.indices.create(index=\"all_papers_sbert\", mappings=index_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_list = df.to_dict(\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for paper in papers_list:\n",
    "    try:\n",
    "        es.index(index=\"all_papers_sbert\", document=paper, id=paper['id'])\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es.count(index=\"all_papers_sbert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_sentence_vectors(model, tokenizer, documents):\n",
    "  input = tokenizer(documents, return_tensors='pt', padding=True, truncation=True)\n",
    "  output = model(**input)\n",
    "  sentence_vec = output.last_hidden_state.mean(dim=1).detach()\n",
    "  sentence_vec = np.squeeze(np.asarray(sentence_vec))\n",
    "  return sentence_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for Keyword...\n",
    "def get_search_results(input_keyword):\n",
    "    input_vec = get_bert_sentence_vectors(model, tokenizer, input_keyword)\n",
    "\n",
    "    query_vector = {\n",
    "        \"field\": \"search_text_vector\",\n",
    "        \"query_vector\": input_vec,\n",
    "        \"k\": 100,\n",
    "        \"num_candidates\": 1000\n",
    "    }\n",
    "\n",
    "    query = {\n",
    "        \"query\": {\n",
    "            \"multi_match\": {\n",
    "                \"query\": input_keyword,\n",
    "                \"fields\": [\"authors^1.0\", \"title^6.0\", \"abstract^4.0\"],\n",
    "                \"operator\": \"and\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    search_results_vector = es.knn_search(index=\"all_papers_sbert\", knn=query_vector, source=['title', 'abstract', 'doi', 'update_date', 'authors'])[\"hits\"][\"hits\"]\n",
    "    search_results_text = es.search(index=\"all_papers_sbert\", body=query, source=['title', 'abstract', 'doi', 'update_date', 'authors'])[\"hits\"][\"hits\"]\n",
    "    \n",
    "    return search_results_vector, search_results_text\n",
    "    \n",
    "def get_pagerank_results(search_results_vector):\n",
    "    num_nodes = len(search_results_vector)\n",
    "    adjacency_matrix = np.zeros((num_nodes, num_nodes))\n",
    "\n",
    "    for hit in search_results_vector:\n",
    "        node_id = int(float(hit['_id']))\n",
    "        neighbors = hit['_source'].get('neighbors', [])\n",
    "        for neighbor_id in neighbors:\n",
    "            neighbor_id = int(neighbor_id)\n",
    "            adjacency_matrix[node_id, neighbor_id] = 1\n",
    "\n",
    "    num_nodes = adjacency_matrix.shape[0]\n",
    "    pagerank_scores = np.ones(num_nodes) / num_nodes\n",
    "    \n",
    "    num_iterations = 100\n",
    "    damping_factor = 0.85\n",
    "\n",
    "    for _ in range(num_iterations):\n",
    "        denom = num_nodes + damping_factor * np.dot(adjacency_matrix, pagerank_scores)\n",
    "        pagerank_scores = (1 - damping_factor) / denom + 0.0001\n",
    "        \n",
    "    ranked_results = []\n",
    "\n",
    "    for result, pagerank_score in zip(search_results_vector, pagerank_scores):\n",
    "        elasticsearch_score = result['_score']\n",
    "        combined_score = elasticsearch_score + pagerank_score\n",
    "        result['_combined_score'] = combined_score\n",
    "        ranked_results.append(result)\n",
    "\n",
    "    ranked_results.sort(key=lambda x: x['_combined_score'], reverse=True)\n",
    "    return ranked_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)\n",
    "cors = CORS(app)\n",
    "app.config['CORS_HEADERS'] = 'Content-Type'\n",
    "\n",
    "@app.route('/search')\n",
    "@cross_origin()\n",
    "def get_results():\n",
    "    search_key = request.args.get('search_key')\n",
    "    search_results_vector, search_results_text = get_search_results(search_key)\n",
    "    search_results = get_pagerank_results(search_results_text) + get_pagerank_results(search_results_vector)\n",
    "    return json.dumps(search_results)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(host='0.0.0.0', port=8080)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
